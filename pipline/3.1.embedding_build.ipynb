{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/p300s/wangmx_group/xutingfeng/SIS/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-52200760334198e7\n",
      "Reusing dataset csv (/home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c59f83973e849caa9725c49e2b244b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-d7b3eee632703ea9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-bec2fd355b0cb7d7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-0f1986b4010acd7d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-fbd879c251d0f77b.arrow\n",
      "Loading cached split indices for dataset at /home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-f445a5304f7ba125.arrow and /home/xutingfeng/.cache/huggingface/datasets/csv/default-52200760334198e7/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-cfda44d28db5f2dc.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sis.dataset import SISDataset\n",
    "import torch \n",
    "import transformers \n",
    "from sis.model.sismodel import DoubleTransformerModel\n",
    "from sis.utils import modelParametersNum, try_gpu\n",
    "\n",
    "# Step1 dataset load \n",
    "# device = try_gpu()\n",
    "device = \"cuda:1\"\n",
    "\n",
    "sisdataset = SISDataset(root_dir=\"/p300s/wangmx_group/xutingfeng/SIS/sis/dataset/total_data.csv\", device = device)\n",
    "\n",
    "aa_vocab = sisdataset.aa_vocab\n",
    "sis_datasetDict = sisdataset.dataset_dict\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(sis_datasetDict[\"train\"], batch_size=4, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(sis_datasetDict[\"test\"], batch_size=4, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.],\n",
       "       [1., 2., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "aa_embedding_dict = {aa:[1, 2, 3] for aa in aa_vocab.get_itos()}\n",
    "\n",
    "aa_token_embedding_dict = {aa_vocab.lookup_indices([aa])[0]:value for aa, value in aa_embedding_dict.items()}\n",
    "\n",
    "sorted_embeeding_tuple = sorted(aa_token_embedding_dict.items(), key = lambda x: x[0]) # (tokens, values) sorted by token at ascending order\n",
    "sorted_embedding_array = np.stack([value for token, value in sorted_embeeding_tuple]).astype(np.float32)\n",
    "sorted_embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_ = F.one_hot(torch.tensor([1, 0]), 22)\n",
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5484, 0.0738, 0.7047, 0.3437, 0.3530],\n",
       "        [0.4896, 0.3018, 0.5982, 0.7547, 0.9737]], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_@sorted_embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import numpy as np \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    EmbeddingLayer 传入aa_embeding_dict和aa_vocab\n",
    "    aa_embedding_dict is {\"A\":[1, 2, 3], ...}\n",
    "    aa_vocab is `torchtext.vocab.vocab.Vocab`\n",
    "\n",
    "    这两个包含的token应该是一致的\n",
    "\n",
    "    Example:\n",
    "        from sis.dataset.vocab import build_vocab_from_alphabet_dict\n",
    "        from sis.dataset.constants import BASE_AMINO_ACIDS\n",
    "\n",
    "        aa_vocab = build_vocab_from_alphabet_dict(BASE_AMINO_ACIDS)\n",
    "        aa_embedding_dict = {aa:np.random.randint(0, 5, size=(5)) for aa in aa_vocab.get_itos()}\n",
    "\n",
    "        EMLayer = EmbeddingLayer(aa_embedding_dict=aa_embedding_dict, aa_vocab=aa_vocab)\n",
    "\n",
    "        EMLayer(torch.tensor([1, 3, 5, 1, 2, 4]))\n",
    "    \"\"\"\n",
    "    def __init__(self, aa_embedding_dict, aa_vocab):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "\n",
    "        if len(aa_embedding_dict.keys()) != len(aa_vocab):\n",
    "            raise ValueError(\"aa_embedding_dict should contain all token in aa_vocab, check whether special tokens are in aa_embedding_dict!\")\n",
    "        \n",
    "        aa_token_embedding_dict = {aa_vocab.lookup_indices([aa])[0]:value for aa, value in aa_embedding_dict.items()}\n",
    "        \n",
    "        sorted_embeeding_tuple = sorted(aa_token_embedding_dict.items(), key = lambda x: x[0]) # (tokens, values) sorted by token at ascending order\n",
    "        sorted_embedding_array = np.stack([value for token, value in sorted_embeeding_tuple]).astype(np.float32)\n",
    "\n",
    "        self.sorted_embeeding_tuple = sorted_embeeding_tuple\n",
    "        self.w = torch.tensor(sorted_embedding_array)\n",
    "        self.d_model = self.w.shape[1]\n",
    "        self.tokens_num = self.w.shape[0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_onehot = F.one_hot(x, self.tokens_num).float()\n",
    "        return x_onehot @ self.w \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': array([2, 2, 1, 4, 1]),\n",
       " '<pad>': array([3, 0, 1, 3, 4]),\n",
       " 'A': array([0, 2, 2, 3, 0]),\n",
       " 'C': array([3, 1, 0, 4, 2]),\n",
       " 'D': array([3, 2, 2, 3, 0]),\n",
       " 'E': array([3, 3, 3, 0, 0]),\n",
       " 'F': array([4, 4, 1, 3, 1]),\n",
       " 'G': array([3, 1, 2, 1, 2]),\n",
       " 'H': array([2, 0, 1, 2, 4]),\n",
       " 'I': array([3, 3, 0, 4, 4]),\n",
       " 'K': array([1, 0, 4, 3, 2]),\n",
       " 'L': array([3, 4, 3, 4, 4]),\n",
       " 'M': array([0, 2, 3, 4, 2]),\n",
       " 'N': array([2, 2, 1, 2, 4]),\n",
       " 'P': array([1, 2, 2, 0, 0]),\n",
       " 'Q': array([4, 1, 0, 3, 4]),\n",
       " 'R': array([4, 3, 1, 2, 2]),\n",
       " 'S': array([2, 1, 4, 4, 2]),\n",
       " 'T': array([4, 1, 2, 4, 4]),\n",
       " 'V': array([3, 1, 2, 0, 0]),\n",
       " 'W': array([2, 4, 1, 3, 1]),\n",
       " 'Y': array([1, 3, 0, 2, 0])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sis.dataset.vocab import build_vocab_from_alphabet_dict\n",
    "from sis.dataset.constants import BASE_AMINO_ACIDS\n",
    "\n",
    "aa_vocab = build_vocab_from_alphabet_dict(BASE_AMINO_ACIDS)\n",
    "aa_embedding_dict = {aa:np.random.randint(0, 5, size=(5)) for aa in aa_vocab.get_itos()}\n",
    "\n",
    "aa_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMLayer = EmbeddingLayer(aa_embedding_dict=aa_embedding_dict, aa_vocab=aa_vocab)\n",
    "EMLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 0., 1., 3., 4.],\n",
       "        [3., 1., 0., 4., 2.],\n",
       "        [3., 3., 3., 0., 0.],\n",
       "        [3., 0., 1., 3., 4.],\n",
       "        [0., 2., 2., 3., 0.],\n",
       "        [3., 2., 2., 3., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EMLayer(torch.tensor([1, 3, 5, 1, 2, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aa_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pka_cooh_alpha</th>\n",
       "      <th>pka_nh3</th>\n",
       "      <th>pka_rgroup</th>\n",
       "      <th>isoelectric_points</th>\n",
       "      <th>molecularweight</th>\n",
       "      <th>numbercodons</th>\n",
       "      <th>bulkiness</th>\n",
       "      <th>polarityzimmerman</th>\n",
       "      <th>polaritygrantham</th>\n",
       "      <th>refractivity</th>\n",
       "      <th>...</th>\n",
       "      <th>coilroux</th>\n",
       "      <th>alpha_helixlevitt</th>\n",
       "      <th>beta_sheetlevitt</th>\n",
       "      <th>beta_turnlevitt</th>\n",
       "      <th>totalbeta_strand</th>\n",
       "      <th>antiparallelbeta_strand</th>\n",
       "      <th>parallelbeta_strand</th>\n",
       "      <th>a_a_composition</th>\n",
       "      <th>a_a_swiss_prot</th>\n",
       "      <th>relativemutability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALA</th>\n",
       "      <td>2.35</td>\n",
       "      <td>9.87</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.11</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.1</td>\n",
       "      <td>4.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.25</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARG</th>\n",
       "      <td>2.18</td>\n",
       "      <td>9.09</td>\n",
       "      <td>13.20</td>\n",
       "      <td>10.76</td>\n",
       "      <td>174.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.28</td>\n",
       "      <td>52.00</td>\n",
       "      <td>10.5</td>\n",
       "      <td>26.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.68</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.53</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASN</th>\n",
       "      <td>2.18</td>\n",
       "      <td>9.09</td>\n",
       "      <td>13.20</td>\n",
       "      <td>10.76</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.82</td>\n",
       "      <td>3.38</td>\n",
       "      <td>11.6</td>\n",
       "      <td>13.28</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.06</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASP</th>\n",
       "      <td>1.88</td>\n",
       "      <td>9.60</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.98</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.68</td>\n",
       "      <td>49.70</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.197</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.45</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CYS</th>\n",
       "      <td>1.71</td>\n",
       "      <td>10.78</td>\n",
       "      <td>8.33</td>\n",
       "      <td>5.02</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>5.5</td>\n",
       "      <td>35.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.37</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLN</th>\n",
       "      <td>2.17</td>\n",
       "      <td>9.13</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.65</td>\n",
       "      <td>146.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.45</td>\n",
       "      <td>3.53</td>\n",
       "      <td>10.5</td>\n",
       "      <td>17.56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.28</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.93</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLU</th>\n",
       "      <td>2.19</td>\n",
       "      <td>9.67</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.08</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.57</td>\n",
       "      <td>49.90</td>\n",
       "      <td>12.3</td>\n",
       "      <td>17.26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.75</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLY</th>\n",
       "      <td>2.34</td>\n",
       "      <td>9.60</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.06</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.79</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.07</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIS</th>\n",
       "      <td>1.78</td>\n",
       "      <td>8.97</td>\n",
       "      <td>5.97</td>\n",
       "      <td>7.64</td>\n",
       "      <td>155.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.69</td>\n",
       "      <td>51.60</td>\n",
       "      <td>10.4</td>\n",
       "      <td>21.81</td>\n",
       "      <td>...</td>\n",
       "      <td>1.068</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.27</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ILE</th>\n",
       "      <td>2.32</td>\n",
       "      <td>9.76</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.04</td>\n",
       "      <td>131.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.2</td>\n",
       "      <td>19.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.60</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.96</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LEU</th>\n",
       "      <td>2.36</td>\n",
       "      <td>9.60</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.04</td>\n",
       "      <td>131.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.40</td>\n",
       "      <td>0.13</td>\n",
       "      <td>4.9</td>\n",
       "      <td>18.78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.42</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LYS</th>\n",
       "      <td>2.20</td>\n",
       "      <td>8.90</td>\n",
       "      <td>10.28</td>\n",
       "      <td>9.47</td>\n",
       "      <td>146.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.71</td>\n",
       "      <td>49.50</td>\n",
       "      <td>11.3</td>\n",
       "      <td>21.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.84</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MET</th>\n",
       "      <td>2.28</td>\n",
       "      <td>9.21</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.74</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.25</td>\n",
       "      <td>1.43</td>\n",
       "      <td>5.7</td>\n",
       "      <td>21.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.42</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHE</th>\n",
       "      <td>2.58</td>\n",
       "      <td>9.24</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.91</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.80</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.2</td>\n",
       "      <td>29.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.30</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.86</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRO</th>\n",
       "      <td>1.99</td>\n",
       "      <td>10.60</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.30</td>\n",
       "      <td>115.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.43</td>\n",
       "      <td>1.58</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SER</th>\n",
       "      <td>2.21</td>\n",
       "      <td>9.15</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.68</td>\n",
       "      <td>105.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.47</td>\n",
       "      <td>1.67</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.70</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.56</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THR</th>\n",
       "      <td>2.15</td>\n",
       "      <td>9.12</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.60</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.77</td>\n",
       "      <td>1.66</td>\n",
       "      <td>8.6</td>\n",
       "      <td>11.01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.59</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.34</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRP</th>\n",
       "      <td>2.38</td>\n",
       "      <td>9.39</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.88</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.67</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.4</td>\n",
       "      <td>42.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.08</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TYR</th>\n",
       "      <td>2.20</td>\n",
       "      <td>9.11</td>\n",
       "      <td>10.07</td>\n",
       "      <td>5.63</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.03</td>\n",
       "      <td>1.61</td>\n",
       "      <td>6.2</td>\n",
       "      <td>31.53</td>\n",
       "      <td>...</td>\n",
       "      <td>1.109</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.08</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.92</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL</th>\n",
       "      <td>2.29</td>\n",
       "      <td>9.74</td>\n",
       "      <td>7.00</td>\n",
       "      <td>6.02</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>5.9</td>\n",
       "      <td>13.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.63</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.87</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pka_cooh_alpha  pka_nh3  pka_rgroup  isoelectric_points  molecularweight  \\\n",
       "ALA            2.35     9.87        7.00                6.11             89.0   \n",
       "ARG            2.18     9.09       13.20               10.76            174.0   \n",
       "ASN            2.18     9.09       13.20               10.76            132.0   \n",
       "ASP            1.88     9.60        3.65                2.98            133.0   \n",
       "CYS            1.71    10.78        8.33                5.02            121.0   \n",
       "GLN            2.17     9.13        7.00                5.65            146.0   \n",
       "GLU            2.19     9.67        4.25                3.08            147.0   \n",
       "GLY            2.34     9.60        7.00                6.06             75.0   \n",
       "HIS            1.78     8.97        5.97                7.64            155.0   \n",
       "ILE            2.32     9.76        7.00                6.04            131.0   \n",
       "LEU            2.36     9.60        7.00                6.04            131.0   \n",
       "LYS            2.20     8.90       10.28                9.47            146.0   \n",
       "MET            2.28     9.21        7.00                5.74            149.0   \n",
       "PHE            2.58     9.24        7.00                5.91            165.0   \n",
       "PRO            1.99    10.60        7.00                6.30            115.0   \n",
       "SER            2.21     9.15        7.00                5.68            105.0   \n",
       "THR            2.15     9.12        7.00                5.60            119.0   \n",
       "TRP            2.38     9.39        7.00                5.88            204.0   \n",
       "TYR            2.20     9.11       10.07                5.63            181.0   \n",
       "VAL            2.29     9.74        7.00                6.02            117.0   \n",
       "\n",
       "     numbercodons  bulkiness  polarityzimmerman  polaritygrantham  \\\n",
       "ALA           4.0      11.50               0.00               8.1   \n",
       "ARG           6.0      14.28              52.00              10.5   \n",
       "ASN           2.0      12.82               3.38              11.6   \n",
       "ASP           2.0      11.68              49.70              13.0   \n",
       "CYS           1.0      13.46               1.48               5.5   \n",
       "GLN           2.0      14.45               3.53              10.5   \n",
       "GLU           2.0      13.57              49.90              12.3   \n",
       "GLY           4.0       3.40               0.00               9.0   \n",
       "HIS           2.0      13.69              51.60              10.4   \n",
       "ILE           3.0      21.40               0.13               5.2   \n",
       "LEU           6.0      21.40               0.13               4.9   \n",
       "LYS           2.0      15.71              49.50              11.3   \n",
       "MET           1.0      16.25               1.43               5.7   \n",
       "PHE           2.0      19.80               0.35               5.2   \n",
       "PRO           4.0      17.43               1.58               8.0   \n",
       "SER           6.0       9.47               1.67               9.2   \n",
       "THR           4.0      15.77               1.66               8.6   \n",
       "TRP           1.0      21.67               2.10               5.4   \n",
       "TYR           2.0      18.03               1.61               6.2   \n",
       "VAL           4.0      21.57               0.13               5.9   \n",
       "\n",
       "     refractivity  ...  coilroux  alpha_helixlevitt  beta_sheetlevitt  \\\n",
       "ALA          4.34  ...     0.824               1.29              0.90   \n",
       "ARG         26.66  ...     0.893               0.96              0.99   \n",
       "ASN         13.28  ...     1.167               0.90              0.76   \n",
       "ASP         12.00  ...     1.197               1.04              0.72   \n",
       "CYS         35.77  ...     0.953               1.11              0.74   \n",
       "GLN         17.56  ...     0.947               1.27              0.80   \n",
       "GLU         17.26  ...     0.761               1.44              0.75   \n",
       "GLY          0.00  ...     1.251               0.56              0.92   \n",
       "HIS         21.81  ...     1.068               1.22              1.08   \n",
       "ILE         19.06  ...     0.886               0.97              1.45   \n",
       "LEU         18.78  ...     0.810               1.30              1.02   \n",
       "LYS         21.29  ...     0.897               1.23              0.77   \n",
       "MET         21.64  ...     0.810               1.47              0.97   \n",
       "PHE         29.40  ...     0.797               1.07              1.32   \n",
       "PRO         10.93  ...     1.540               0.52              0.64   \n",
       "SER          6.35  ...     1.130               0.82              0.95   \n",
       "THR         11.01  ...     1.148               0.82              1.21   \n",
       "TRP         42.53  ...     0.941               0.99              1.14   \n",
       "TYR         31.53  ...     1.109               0.72              1.25   \n",
       "VAL         13.92  ...     0.772               0.91              1.49   \n",
       "\n",
       "     beta_turnlevitt  totalbeta_strand  antiparallelbeta_strand  \\\n",
       "ALA             0.77              0.92                     0.90   \n",
       "ARG             0.88              0.93                     1.02   \n",
       "ASN             1.28              0.60                     0.62   \n",
       "ASP             1.41              0.48                     0.47   \n",
       "CYS             0.81              1.16                     1.24   \n",
       "GLN             0.98              0.95                     1.18   \n",
       "GLU             0.99              0.61                     0.62   \n",
       "GLY             1.64              0.61                     0.56   \n",
       "HIS             0.68              0.93                     1.12   \n",
       "ILE             0.51              1.81                     1.54   \n",
       "LEU             0.58              1.30                     1.26   \n",
       "LYS             0.96              0.70                     0.74   \n",
       "MET             0.41              1.19                     1.09   \n",
       "PHE             0.59              1.25                     1.23   \n",
       "PRO             1.91              0.40                     0.42   \n",
       "SER             1.32              0.82                     0.87   \n",
       "THR             1.04              1.12                     1.30   \n",
       "TRP             0.76              1.54                     1.75   \n",
       "TYR             1.05              1.53                     1.68   \n",
       "VAL             0.47              1.81                     1.53   \n",
       "\n",
       "     parallelbeta_strand  a_a_composition  a_a_swiss_prot  relativemutability  \n",
       "ALA                 1.00              8.3            8.25               100.0  \n",
       "ARG                 0.68              5.7            5.53                65.0  \n",
       "ASN                 0.54              4.4            4.06               134.0  \n",
       "ASP                 0.50              5.3            5.45               106.0  \n",
       "CYS                 0.91              1.7            1.37                20.0  \n",
       "GLN                 0.28              4.0            3.93                93.0  \n",
       "GLU                 0.59              6.2            6.75               102.0  \n",
       "GLY                 0.79              7.2            7.07                49.0  \n",
       "HIS                 0.38              2.2            2.27                66.0  \n",
       "ILE                 2.60              5.2            5.96                96.0  \n",
       "LEU                 1.42              9.0            9.66                40.0  \n",
       "LYS                 0.59              5.7            5.84                56.0  \n",
       "MET                 1.49              2.4            2.42                94.0  \n",
       "PHE                 1.30              3.9            3.86                41.0  \n",
       "PRO                 0.35              5.1            4.70                56.0  \n",
       "SER                 0.70              6.9            6.56               120.0  \n",
       "THR                 0.59              5.8            5.34                97.0  \n",
       "TRP                 0.89              1.3            1.08                18.0  \n",
       "TYR                 1.08              3.2            2.92                41.0  \n",
       "VAL                 2.63              6.6            6.87                74.0  \n",
       "\n",
       "[20 rows x 61 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/p300s/wangmx_group/xutingfeng/SIS/sis/dataset/EmbeddingData/amino_acid_properties.csv\"\n",
    "\n",
    "expasy = pd.read_csv(path, sep=\",\",index_col=0).T\n",
    "expasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sis.dataset.EmbeddingData.parse import load_expasy_embedding_dict, load_meiler_embedding_dict\n",
    "\n",
    "from sis.dataset.vocab import build_vocab_from_alphabet_dict\n",
    "from sis.dataset.constants import BASE_AMINO_ACIDS\n",
    "from sis.model.embedding import EmbeddingLayer\n",
    "\n",
    "aa_embedding_dict = load_expasy_embedding_dict()\n",
    "aa_vocab = build_vocab_from_alphabet_dict(BASE_AMINO_ACIDS)\n",
    "\n",
    "EMLayer = EmbeddingLayer(aa_embedding_dict=aa_embedding_dict, aa_vocab=aa_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
